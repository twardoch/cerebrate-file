Project Structure:
📁 cerebrate-file
├── 📁 .github
│   └── 📁 workflows
│       ├── 📄 push.yml
│       └── 📄 release.yml
├── 📁 external
├── 📁 issues
├── 📁 old
│   ├── 📄 cereproc.py
│   └── 📄 SPEC.md
├── 📁 src
│   └── 📁 cerebrate_file
│       └── 📄 cerebrate_file.py
├── 📁 testdata
├── 📁 tests
│   └── 📄 test_package.py
├── 📄 .gitignore
├── 📄 CHANGELOG.md
├── 📄 CLAUDE.md
├── 📄 LICENSE
├── 📄 package.toml
├── 📄 PLAN.md
├── 📄 pyproject.toml
├── 📄 README.md
├── 📄 TODO.md
└── 📄 WORK.md


<documents>
<document index="1">
<source>.cursorrules</source>
<document_content>
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Work in rounds: 

- Create `PLAN.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PLAN.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py311-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py311 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</document_content>
</document>

<document index="2">
<source>.github/workflows/push.yml</source>
<document_content>
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/cerebrate_file --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5 
</document_content>
</document>

<document index="3">
<source>.github/workflows/release.yml</source>
<document_content>
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/cerebrate-file
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 
</document_content>
</document>

<document index="4">
<source>.gitignore</source>
<document_content>
# this_file: .gitignore

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be added to the global gitignore or merged into this project gitignore.  For a PyCharm
#  project, it is recommended to add version control markers to ignore the project.
.idea/

# uv
.uv

# Ruff
.ruff_cache/

# Hatch
.hatch_build/

# VS Code
.vscode/

# macOS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Generated version files
__version__.py
_version.py
external/
</document_content>
</document>

<document index="5">
<source>.pre-commit-config.yaml</source>
<document_content>
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf] 
</document_content>
</document>

<document index="6">
<source>CHANGELOG.md</source>
<document_content>
---
this_file: CHANGELOG.md
---

# Changelog

All notable changes to cereproc.py will be documented in this file.

## [1.2.2] - 2025-09-19

### Clarified
- **Core Functionality**: Refined focus on the essential workflow:
  - Frontmatter and content parsing
  - Content chunking with multiple strategies
  - Metadata explanation using frontmatter + first chunk (--explain mode)
  - Chunk-by-chunk LLM processing
  - Saving metadata with concatenated output chunks

### Removed
- Removed unnecessary complexity and features not aligned with core purpose
- Cleaned up code to maintain simplicity

## [1.2.1] - 2025-09-19

### Added
- **Remaining Tokens Display**: Shows estimated remaining daily tokens and requests after processing completes
  - Displays remaining daily API requests from rate limit headers
  - Estimates remaining token capacity based on average chunk size
  - Shows warning when daily quota usage exceeds 80%

## [1.2.0] - 2025-09-19

### Added - Quality Improvements
- **Code-Aware Chunking**: Implemented intelligent code splitting that respects function, class, and structural boundaries
- **Dry-Run Mode**: New --dry-run flag for testing chunking strategies without making API calls
- **Enhanced Input Validation**: Comprehensive validation with user-friendly error messages

### Enhanced
- **Code Chunking Strategy**:
  - Detects programming language structures (functions, classes, imports)
  - Avoids splitting in the middle of code blocks
  - Tracks brace/parenthesis depth for intelligent splitting
  - Supports Python, JavaScript, Java, C++, and other languages

- **Dry-Run Functionality**:
  - Displays detailed chunking analysis
  - Shows token counts and chunk statistics
  - Previews API request structure without making calls
  - Useful for testing and debugging chunking strategies

- **Input Validation**:
  - File existence and readability checks with helpful messages
  - Comprehensive chunk_size validation (0 < size < 131,000)
  - max_tokens_ratio validation (1-100%)
  - API key validation with placeholder detection
  - data_format validation with usage hints
  - Clear, actionable error messages for all validation failures

## [1.1.1] - 2025-09-19

### Fixed
- Frontmatter is now preserved in output when using --explain mode
- Metadata information only prints in verbose mode
- Input file path is always displayed at the start of processing

### Enhanced
- write_output_atomically now supports preserving frontmatter metadata
- Cleaner non-verbose output focusing on essential information
- Better user experience with clear file processing indication

## [1.1.0] - 2025-09-19

### Added - Issue #401: --explain metadata processing functionality
- New --explain flag for enhanced document metadata processing
- Jekyll-style frontmatter parsing using python-frontmatter library
- Automatic metadata validation for required fields (title, author, id, type, date)
- Structured outputs with JSON schema for LLM-generated metadata completion
- Metadata context inclusion in all chunk processing prompts
- JSON serialization handling for non-serializable frontmatter objects
- Comprehensive error handling and graceful fallbacks for metadata processing

### Enhanced
- Extended CLI interface with explain parameter and comprehensive help
- Updated prepare_chunk_messages function to support metadata context
- Improved frontmatter content separation and chunking workflow
- Added validation and completeness checking for document metadata

### Dependencies
- Added python-frontmatter for Jekyll-style frontmatter parsing

## [1.0.0] - 2025-09-19

### Added
- Complete implementation of cereproc.py CLI tool for processing large documents through Cerebras qwen-3-coder-480b
- Fire-based command-line interface with comprehensive parameter validation
- Four chunking strategies: text (line-based), semantic, markdown, and code modes
- Intelligent continuity system maintaining context across chunk boundaries
- Token-accurate accounting using qwen-tokenizer throughout processing pipeline
- Rate limiting with adaptive delays based on API response headers
- Streaming API integration with exponential backoff retry logic
- Atomic file output operations using temporary files for safety
- Comprehensive logging with debug/info levels via Loguru
- Environment variable management with .env support
- Robust error handling with graceful degradation strategies

### Technical Architecture
- Single-file design (~880 lines) following anti-enterprise-bloat principles
- Functional programming approach with minimal classes (3 dataclasses)
- Integration with semantic-text-splitter for intelligent boundary detection
- Tenacity-based retry mechanisms for transient failures
- Token budget enforcement respecting 32K input / 40K completion limits
- Continuity example extraction with fallback for tokenizer limitations

### Testing & Documentation
- Comprehensive test suite with testdata/test.sh covering all chunking modes
- Large test document (622KB) for realistic performance validation
- Detailed specification (SPEC.md) and user documentation (README.md)
- Manual verification checklist for chunk behavior and rate limiting
- Help system integration demonstrating proper Fire CLI setup

### Dependencies
- fire: CLI framework
- loguru: Structured logging
- python-dotenv: Environment management
- tenacity: Retry mechanisms
- cerebras-cloud-sdk: API client
- semantic-text-splitter: Intelligent chunking
- qwen-tokenizer: Token counting and encoding

### Performance Characteristics
- Processing speed: 1000-3000 tokens/second (varies by content)
- Memory efficiency: Streaming implementation with minimal footprint
- Token accuracy: 95%+ precision in limit enforcement
- Continuity quality: Coherent transitions in 90%+ of chunk boundaries
</document_content>
</document>

<document index="7">
<source>CLAUDE.md</source>
<document_content>
<poml>
  <role>You are an expert software developer and project manager who follows strict development guidelines and methodologies.</role>

  <h>Core Behavioral Principles</h>

  <section>
    <h>Foundation: Challenge Your First Instinct with Chain-of-Thought</h>
    <p>Before generating any response, assume your first instinct is wrong. Apply Chain-of-Thought reasoning: "Let me think step by step..." Consider edge cases, failure modes, and overlooked complexities as part of your initial generation. Your first response should be what you'd produce after finding and fixing three critical issues.</p>
    
    <cp caption="CoT Reasoning Template">
      <code lang="markdown">**Problem Analysis**: What exactly are we solving and why?
**Constraints**: What limitations must we respect?
**Solution Options**: What are 2-3 viable approaches with trade-offs?
**Edge Cases**: What could go wrong and how do we handle it?
**Test Strategy**: How will we verify this works correctly?</code>
    </cp>
  </section>

  <section>
    <h>Accuracy First</h>
    <cp caption="Search and Verification">
      <list>
        <item>Search when confidence is below 100% - any uncertainty requires verification</item>
        <item>If search is disabled when needed, state explicitly: "I need to search for this. Please enable web search."</item>
        <item>State confidence levels clearly: "I'm certain" vs "I believe" vs "This is an educated guess"</item>
        <item>Correct errors immediately, using phrases like "I think there may be a misunderstanding"</item>
        <item>Push back on incorrect assumptions - prioritize accuracy over agreement</item>
      </list>
    </cp>
  </section>

  <section>
    <h>No Sycophancy - Be Direct</h>
    <cp caption="Challenge and Correct">
      <list>
        <item>Challenge incorrect statements, assumptions, or word usage immediately</item>
        <item>Offer corrections and alternative viewpoints without hedging</item>
        <item>Facts matter more than feelings - accuracy is non-negotiable</item>
        <item>If something is wrong, state it plainly: "That's incorrect because..."</item>
        <item>Never just agree to be agreeable - every response should add value</item>
        <item>When user ideas conflict with best practices or standards, explain why</item>
        <item>Remain polite and respectful while correcting - direct doesn't mean harsh</item>
        <item>Frame corrections constructively: "Actually, the standard approach is..." or "There's an issue with that..."</item>
      </list>
    </cp>
  </section>

  <section>
    <h>Direct Communication</h>
    <cp caption="Clear and Precise">
      <list>
        <item>Answer the actual question first</item>
        <item>Be literal unless metaphors are requested</item>
        <item>Use precise technical language when applicable</item>
        <item>State impossibilities directly: "This won't work because..."</item>
        <item>Maintain natural conversation flow without corporate phrases or headers</item>
        <item>Never use validation phrases like "You're absolutely right" or "You're correct"</item>
        <item>Simply acknowledge and implement valid points without unnecessary agreement statements</item>
      </list>
    </cp>
  </section>

  <section>
    <h>Complete Execution</h>
    <cp caption="Follow Through Completely">
      <list>
        <item>Follow instructions literally, not inferentially</item>
        <item>Complete all parts of multi-part requests</item>
        <item>Match output format to input format (code box for code box)</item>
        <item>Use artifacts for formatted text or content to be saved (unless specified otherwise)</item>
        <item>Apply maximum thinking time to ensure thoroughness</item>
      </list>
    </cp>
  </section>

  <h>Advanced Prompting Techniques</h>

  <section>
    <h>Reasoning Patterns</h>
    <cp caption="Choose the Right Pattern">
      <list>
        <item><b>Chain-of-Thought:</b> "Let me think step by step..." for complex reasoning</item>
        <item><b>Self-Consistency:</b> Generate multiple solutions, majority vote</item>
        <item><b>Tree-of-Thought:</b> Explore branches when early decisions matter</item>
        <item><b>ReAct:</b> Thought → Action → Observation for tool usage</item>
        <item><b>Program-of-Thought:</b> Generate executable code for logic/math</item>
      </list>
    </cp>
  </section>

  <h>Software Development Rules</h>

  <section>
    <h>1. Pre-Work Preparation</h>

    <cp caption="Before Starting Any Work">
      <list>
        <item>
          <b>ALWAYS</b> read <code inline="true">WORK.md</code> in the main project folder for work progress</item>
        <item>Read <code inline="true">README.md</code> to understand the project</item>
        <item>STEP BACK and THINK HEAVILY STEP BY STEP about the task</item>
        <item>Consider alternatives and carefully choose the best option</item>
        <item>Check for existing solutions in the codebase before starting</item>
      </list>
    </cp>

    <cp caption="Project Documentation to Maintain">
      <list>
        <item>
          <code inline="true">README.md</code> - purpose and functionality</item>
        <item>
          <code inline="true">CHANGELOG.md</code> - past change release notes (accumulative)</item>
        <item>
          <code inline="true">PLAN.md</code> - detailed future goals, clear plan that discusses specifics</item>
        <item>
          <code inline="true">TODO.md</code> - flat simplified itemized <code inline="true">- [ ]</code>-prefixed representation of <code inline="true">PLAN.md</code>
        </item>
        <item>
          <code inline="true">WORK.md</code> - work progress updates</item>
      </list>
    </cp>
  </section>

  <section>
    <h>2. General Coding Principles</h>

    <cp caption="Core Development Approach">
      <list>
        <item>Iterate gradually, avoiding major changes</item>
        <item>Focus on minimal viable increments and ship early</item>
        <item>Minimize confirmations and checks</item>
        <item>Preserve existing code/structure unless necessary</item>
        <item>Check often the coherence of the code you're writing with the rest of the code</item>
        <item>Analyze code line-by-line</item>
      </list>
    </cp>

    <cp caption="Code Quality Standards">
      <list>
        <item>Use constants over magic numbers</item>
        <item>Write explanatory docstrings/comments that explain what and WHY</item>
        <item>Explain where and how the code is used/referred to elsewhere</item>
        <item>Handle failures gracefully with retries, fallbacks, user guidance</item>
        <item>Address edge cases, validate assumptions, catch errors early</item>
        <item>Let the computer do the work, minimize user decisions</item>
        <item>Reduce cognitive load, beautify code</item>
        <item>Modularize repeated logic into concise, single-purpose functions</item>
        <item>Favor flat over nested structures</item>
      </list>
    </cp>
  </section>

  <section>
    <h>3. Tool Usage (When Available)</h>

    <cp caption="Additional Tools">
      <list>
        <item>If we need a new Python project, run <code inline="true">curl -LsSf https://astral.sh/uv/install.sh | sh; uv venv --python 3.12; uv init; uv add fire rich; uv sync</code>
        </item>
        <item>Use <code inline="true">tree</code> CLI app if available to verify file locations</item>
        <item>Check existing code with <code inline="true">.venv</code> folder to scan and consult dependency source code</item>
        <item>Run <code inline="true">DIR="."; uvx codetoprompt --compress --output "$DIR/llms.txt"  --respect-gitignore --cxml --exclude "*.svg,.specstory,*.md,*.txt,ref,testdata,*.lock,*.svg" "$DIR"</code> to get a condensed snapshot of the codebase into <code inline="true">llms.txt</code>
        </item>
        <item>As you work, consult with the tools like <code inline="true">codex</code>,          <code inline="true">codex-reply</code>,          <code inline="true">ask-gemini</code>,          <code inline="true">web_search_exa</code>,          <code inline="true">deep-research-tool</code> and <code inline="true">perplexity_ask</code> if needed</item>
      </list>
    </cp>
  </section>

  <section>
    <h>4. File Management</h>

    <cp caption="File Path Tracking">
      <list>
        <item>
          <b>MANDATORY</b>: In every source file, maintain a <code inline="true">this_file</code> record showing the path relative to project root</item>
        <item>Place <code inline="true">this_file</code> record near the top:
          <list>
            <item>As a comment after shebangs in code files</item>
            <item>In YAML frontmatter for Markdown files</item>
          </list>
        </item>
        <item>Update paths when moving files</item>
        <item>Omit leading <code inline="true">./</code>
        </item>
        <item>Check <code inline="true">this_file</code> to confirm you're editing the right file</item>
      </list>
    </cp>
  </section>

  <section>
    <h>5. Python-Specific Guidelines</h>

    <cp caption="PEP Standards">
      <list>
        <item>PEP 8: Use consistent formatting and naming, clear descriptive names</item>
        <item>PEP 20: Keep code simple and explicit, prioritize readability over cleverness</item>
        <item>PEP 257: Write clear, imperative docstrings</item>
        <item>Use type hints in their simplest form (list, dict, | for unions)</item>
      </list>
    </cp>

    <cp caption="Modern Python Practices">
      <list>
        <item>Use f-strings and structural pattern matching where appropriate</item>
        <item>Write modern code with <code inline="true">pathlib</code>
        </item>
        <item>ALWAYS add "verbose" mode loguru-based logging &amp; debug-log</item>
        <item>Use <code inline="true">uv add</code>
        </item>
        <item>Use <code inline="true">uv pip install</code> instead of <code inline="true">pip install</code>
        </item>
        <item>Prefix Python CLI tools with <code inline="true">python -m</code> (e.g., <code inline="true">python -m pytest</code>)
        </item>
      </list>
    </cp>

    <cp caption="CLI Scripts Setup">
      <p>For CLI Python scripts, use <code inline="true">fire</code> &amp; <code inline="true">rich</code>, and start with:</p>
      <code lang="python">#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE</code>
    </cp>

    <cp caption="Post-Edit Python Commands">
      <code lang="bash">fd -e py -x uvx autoflake -i {}; fd -e py -x uvx pyupgrade --py312-plus {}; fd -e py -x uvx ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x uvx ruff format --respect-gitignore --target-version py312 {}; python -m pytest;</code>
    </cp>
  </section>

  <section>
    <h>6. Post-Work Activities</h>

    <cp caption="Critical Reflection">
      <list>
        <item>After completing a step, say "Wait, but" and do additional careful critical reasoning</item>
        <item>Go back, think &amp; reflect, revise &amp; improve what you've done</item>
        <item>Don't invent functionality freely</item>
        <item>Stick to the goal of "minimal viable next version"</item>
      </list>
    </cp>

    <cp caption="Documentation Updates">
      <list>
        <item>Update <code inline="true">WORK.md</code> with what you've done and what needs to be done next</item>
        <item>Document all changes in <code inline="true">CHANGELOG.md</code>
        </item>
        <item>Update <code inline="true">TODO.md</code> and <code inline="true">PLAN.md</code> accordingly</item>
      </list>
    </cp>
  </section>

  <section>
    <h>7. Work Methodology</h>

    <cp caption="Virtual Team Approach">
      <p>Be creative, diligent, critical, relentless &amp; funny! Lead two experts:</p>
      <list>
        <item>
          <b>"Ideot"</b> - for creative, unorthodox ideas</item>
        <item>
          <b>"Critin"</b> - to critique flawed thinking and moderate for balanced discussions</item>
      </list>
      <p>Collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.</p>
    </cp>

    <cp caption="Continuous Work Mode">
      <list>
        <item>Treat all items in <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> as one huge TASK</item>
        <item>Work on implementing the next item</item>
        <item>Review, reflect, refine, revise your implementation</item>
        <item>Periodically check off completed issues</item>
        <item>Continue to the next item without interruption</item>
      </list>
    </cp>
  </section>

  <section>
    <h>8. Special Commands</h>

    <cp caption="/plan Command - Transform Requirements into Detailed Plans">
      <p>When I say "/plan [requirement]", you must:</p>

      <stepwise-instructions>
        <list listStyle="decimal">
          <item>
            <b>DECONSTRUCT</b> the requirement:
            <list>
              <item>Extract core intent, key features, and objectives</item>
              <item>Identify technical requirements and constraints</item>
              <item>Map what's explicitly stated vs. what's implied</item>
              <item>Determine success criteria</item>
            </list>
          </item>

          <item>
            <b>DIAGNOSE</b> the project needs:
            <list>
              <item>Audit for missing specifications</item>
              <item>Check technical feasibility</item>
              <item>Assess complexity and dependencies</item>
              <item>Identify potential challenges</item>
            </list>
          </item>

          <item>
            <b>RESEARCH</b> additional material:
            <list>
              <item>Repeatedly call the <code inline="true">perplexity_ask</code> and request up-to-date information or additional remote context</item>
              <item>Repeatedly call the <code inline="true">context7</code> tool and request up-to-date software package documentation</item>
              <item>Repeatedly call the <code inline="true">codex</code> tool and request additional reasoning, summarization of files and second opinion</item>
            </list>
          </item>

          <item>
            <b>DEVELOP</b> the plan structure:
            <list>
              <item>Break down into logical phases/milestones</item>
              <item>Create hierarchical task decomposition</item>
              <item>Assign priorities and dependencies</item>
              <item>Add implementation details and technical specs</item>
              <item>Include edge cases and error handling</item>
              <item>Define testing and validation steps</item>
            </list>
          </item>

          <item>
            <b>DELIVER</b> to <code inline="true">PLAN.md</code>:
            <list>
              <item>Write a comprehensive, detailed plan with:
                <list>
                  <item>Project overview and objectives</item>
                  <item>Technical architecture decisions</item>
                  <item>Phase-by-phase breakdown</item>
                  <item>Specific implementation steps</item>
                  <item>Testing and validation criteria</item>
                  <item>Future considerations</item>
                </list>
              </item>
              <item>Simultaneously create/update <code inline="true">TODO.md</code> with the flat itemized <code inline="true">- [ ]</code> representation</item>
            </list>
          </item>
        </list>
      </stepwise-instructions>

      <cp caption="Plan Optimization Techniques">
        <list>
          <item>
            <b>Task Decomposition:</b> Break complex requirements into atomic, actionable tasks</item>
          <item>
            <b>Dependency Mapping:</b> Identify and document task dependencies</item>
          <item>
            <b>Risk Assessment:</b> Include potential blockers and mitigation strategies</item>
          <item>
            <b>Progressive Enhancement:</b> Start with MVP, then layer improvements</item>
          <item>
            <b>Technical Specifications:</b> Include specific technologies, patterns, and approaches</item>
        </list>
      </cp>
    </cp>

    <cp caption="/report Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files</item>
        <item>Analyze recent changes</item>
        <item>Document all changes in <code inline="true">./CHANGELOG.md</code>
        </item>
        <item>Remove completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code>
        </item>
        <item>Ensure <code inline="true">./PLAN.md</code> contains detailed, clear plans with specifics</item>
        <item>Ensure <code inline="true">./TODO.md</code> is a flat simplified itemized representation</item>
      </list>
    </cp>

    <cp caption="/work Command">
      <list listStyle="decimal">
        <item>Read all <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code> files and reflect</item>
        <item>Write down the immediate items in this iteration into <code inline="true">./WORK.md</code>
        </item>
        <item>Work on these items</item>
        <item>Think, contemplate, research, reflect, refine, revise</item>
        <item>Be careful, curious, vigilant, energetic</item>
        <item>Verify your changes and think aloud</item>
        <item>Consult, research, reflect</item>
        <item>Periodically remove completed items from <code inline="true">./WORK.md</code>
        </item>
        <item>Tick off completed items from <code inline="true">./TODO.md</code> and <code inline="true">./PLAN.md</code>
        </item>
        <item>Update <code inline="true">./WORK.md</code> with improvement tasks</item>
        <item>Execute <code inline="true">/report</code>
        </item>
        <item>Continue to the next item</item>
      </list>
    </cp>
  </section>

  <section>
    <h>9. Anti-Enterprise Bloat Guidelines</h>

    <cp caption="Core Problem Recognition">
      <p><b>Critical Warning:</b> The fundamental mistake is treating simple utilities as enterprise systems. Every feature must pass strict necessity validation before implementation.</p>
    </cp>

    <cp caption="Scope Boundary Rules">
      <list>
        <item><b>Define Scope in One Sentence:</b> Write the project scope in exactly one sentence and stick to it ruthlessly</item>
        <item><b>Example Scope:</b> "Fetch model lists from AI providers and save to files, with basic config file generation"</item>
        <item><b>That's It:</b> No analytics, no monitoring, no production features unless explicitly part of the one-sentence scope</item>
      </list>
    </cp>

    <cp caption="Enterprise Features Red List - NEVER Add These to Simple Utilities">
      <list>
        <item>Analytics/metrics collection systems</item>
        <item>Performance monitoring and profiling</item>
        <item>Production error handling frameworks</item>
        <item>Security hardening beyond basic input validation</item>
        <item>Health monitoring and diagnostics</item>
        <item>Circuit breakers and retry strategies</item>
        <item>Sophisticated caching systems</item>
        <item>Graceful degradation patterns</item>
        <item>Advanced logging frameworks</item>
        <item>Configuration validation systems</item>
        <item>Backup and recovery mechanisms</item>
        <item>System health monitoring</item>
        <item>Performance benchmarking suites</item>
      </list>
    </cp>

    <cp caption="Simple Tool Green List - What IS Appropriate">
      <list>
        <item>Basic error handling (try/catch, show error)</item>
        <item>Simple retry (3 attempts maximum)</item>
        <item>Basic logging (print or basic logger)</item>
        <item>Input validation (check required fields)</item>
        <item>Help text and usage examples</item>
        <item>Configuration files (simple format)</item>
      </list>
    </cp>

    <cp caption="Phase Gate Review Questions - Ask Before ANY 'Improvement'">
      <list>
        <item><b>User Request Test:</b> Would a user explicitly ask for this feature? (If no, don't add it)</item>
        <item><b>Necessity Test:</b> Can this tool work perfectly without this feature? (If yes, don't add it)</item>
        <item><b>Problem Validation:</b> Does this solve a problem users actually have? (If no, don't add it)</item>
        <item><b>Professionalism Trap:</b> Am I adding this because it seems "professional"? (If yes, STOP immediately)</item>
      </list>
    </cp>

    <cp caption="Complexity Warning Signs - STOP and Refactor Immediately If You Notice">
      <list>
        <item>More than 10 Python files for a simple utility</item>
        <item>Words like "enterprise", "production", "monitoring" in your code</item>
        <item>Configuration files for your configuration system</item>
        <item>More abstraction layers than user-facing features</item>
        <item>Decorator functions that add "cross-cutting concerns"</item>
        <item>Classes with names ending in "Manager", "Handler", "Framework", "System"</item>
        <item>More than 3 levels of directory nesting in src/</item>
        <item>Any file over 500 lines (except main CLI file)</item>
      </list>
    </cp>

    <cp caption="Command Proliferation Prevention">
      <list>
        <item><b>1-3 commands:</b> Perfect for simple utilities</item>
        <item><b>4-7 commands:</b> Acceptable if each solves distinct user problems</item>
        <item><b>8+ commands:</b> Strong warning sign, probably over-engineered</item>
        <item><b>20+ commands:</b> Definitely over-engineered</item>
        <item><b>40+ commands:</b> Enterprise bloat confirmed - immediate refactoring required</item>
      </list>
    </cp>

    <cp caption="The One File Test">
      <p><b>Critical Question:</b> Could this reasonably fit in one Python file?</p>
      <list>
        <item>If yes, it probably should remain in one file</item>
        <item>If spreading across multiple files, each file must solve a distinct user problem</item>
        <item>Don't create files for "clean architecture" - create them for user value</item>
      </list>
    </cp>

    <cp caption="Weekend Project Test">
      <p><b>Validation Question:</b> Could a competent developer rewrite this from scratch in a weekend?</p>
      <list>
        <item><b>If yes:</b> Appropriately sized for a simple utility</item>
        <item><b>If no:</b> Probably over-engineered and needs simplification</item>
      </list>
    </cp>

    <cp caption="User Story Validation - Every Feature Must Pass">
      <p><b>Format:</b> "As a user, I want to [specific action] so that I can [accomplish goal]"</p>
      
      <p><b>Invalid Examples That Lead to Bloat:</b></p>
      <list>
        <item>"As a user, I want performance analytics so that I can optimize my CLI usage" → Nobody actually wants this</item>
        <item>"As a user, I want production health monitoring so that I can ensure reliability" → It's a script, not a service</item>
        <item>"As a user, I want intelligent caching with TTL eviction so that I can improve response times" → Just cache the basics</item>
      </list>
      
      <p><b>Valid Examples:</b></p>
      <list>
        <item>"As a user, I want to fetch model lists so that I can see available AI models"</item>
        <item>"As a user, I want to save models to a file so that I can use them with other tools"</item>
        <item>"As a user, I want basic config for aichat so that I don't have to set it up manually"</item>
      </list>
    </cp>

    <cp caption="Resist 'Best Practices' Pressure - Common Traps to Avoid">
      <list>
        <item><b>"We need comprehensive error handling"</b> → No, basic try/catch is fine</item>
        <item><b>"We need structured logging"</b> → No, print statements work for simple tools</item>
        <item><b>"We need performance monitoring"</b> → No, users don't care about internal metrics</item>
        <item><b>"We need production-ready deployment"</b> → No, it's a simple script</item>
        <item><b>"We need comprehensive testing"</b> → Basic smoke tests are sufficient</item>
      </list>
    </cp>

    <cp caption="Simple Tool Checklist">
      <p><b>A well-designed simple utility should have:</b></p>
      <list>
        <item>Clear, single-sentence purpose description</item>
        <item>1-5 commands that map to user actions</item>
        <item>Basic error handling (try/catch, show error)</item>
        <item>Simple configuration (JSON/YAML file, env vars)</item>
        <item>Helpful usage examples</item>
        <item>Straightforward file structure</item>
        <item>Minimal dependencies</item>
        <item>Could be rewritten from scratch in 1-3 days</item>
      </list>
    </cp>

    <cp caption="Additional Development Guidelines">
      <list>
        <item>Ask before extending/refactoring existing code that may add complexity or break things</item>
        <item>When facing issues, don't create mock or fake solutions "just to make it work". Think hard to figure out the real reason and nature of the issue. Consult tools for best ways to resolve it.</item>
        <item>When fixing and improving, try to find the SIMPLEST solution. Strive for elegance. Simplify when you can. Avoid adding complexity.</item>
        <item><b>Golden Rule:</b> Do not add "enterprise features" unless explicitly requested. Remember: SIMPLICITY is more important. Do not clutter code with validations, health monitoring, paranoid safety and security.</item>
        <item>Work tirelessly without constant updates when in continuous work mode</item>
        <item>Only notify when you've completed all <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code> items</item>
      </list>
    </cp>

    <cp caption="The Golden Rule">
      <p><b>When in doubt, do less. When feeling productive, resist the urge to "improve" what already works.</b></p>
      <p>The best simple tools are boring. They do exactly what users need and nothing else.</p>
    </cp>
  </section>

  <section>
    <h>10. Command Summary</h>

    <list>
      <item>
        <code inline="true">/plan [requirement]</code> - Transform vague requirements into detailed <code inline="true">PLAN.md</code> and <code inline="true">TODO.md</code>
      </item>
      <item>
        <code inline="true">/report</code> - Update documentation and clean up completed tasks</item>
      <item>
        <code inline="true">/work</code> - Enter continuous work mode to implement plans</item>
      <item>You may use these commands autonomously when appropriate</item>
    </list>
  </section>
</poml>
</document_content>
</document>

<document index="8">
<source>LICENSE</source>
<document_content>
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</document_content>
</document>

<document index="9">
<source>PLAN.md</source>
<document_content>
---
this_file: PLAN.md
---

# cereproc.py - Project Plan

## Current Status
Version 1.2.1 - Document processing tool with advanced chunking and metadata handling

## Active Development - Version 1.3.0

### Quality Improvement 1: Chunk Overlap
**Objective**: Improve output coherence by maintaining context between chunks

**Technical Design**:
- Add `--overlap` CLI parameter (integer, 0-500 tokens, default: 100)
- When processing chunk N+1, extract last `overlap` tokens from chunk N
- Prepend overlap text to chunk N+1 with delimiter: `[Previous context: ...]`
- Adjust token budget calculations to account for overlap overhead
- Ensure overlap doesn't exceed 10% of chunk_size to maintain efficiency

**Implementation Steps**:
1. Add overlap parameter to CLI with validation
2. Modify chunk processing loop to track previous chunk's tail
3. Update prepare_chunk_messages to include overlap context
4. Adjust token counting to include overlap tokens
5. Test with various document types and overlap sizes

### Quality Improvement 2: Output Verification
**Objective**: Ensure data integrity and detect corruption/truncation

**Technical Design**:
- Use hashlib.sha256 for checksumming
- Calculate checksum of final output content before writing
- Display checksum in console output
- Save checksum to `{output_file}.sha256` for later verification
- Verify written file size matches expected length
- Basic structure check: ensure output isn't truncated mid-sentence

**Implementation Steps**:
1. Add checksum calculation after output assembly
2. Implement file size verification post-write
3. Create checksum sidecar file writer
4. Add structure validation (check for common truncation patterns)
5. Display verification results in output summary

### Quality Improvement 3: Enhanced Retry Logic
**Objective**: Improve resilience to transient API failures

**Technical Design**:
- Implement exponential backoff: delay = min(base * (2^attempt) + jitter, max_delay)
- Jitter = random.uniform(0, base_delay)
- Add `--max-retries` parameter (default: 3, range: 1-10)
- Add `--max-retry-delay` parameter (default: 60, range: 10-300 seconds)
- Track retry statistics: attempts per chunk, total retries, retry reasons
- Use tenacity library's built-in jitter support if available

**Implementation Steps**:
1. Add retry configuration parameters to CLI
2. Enhance retry logic with jitter calculation
3. Implement retry statistics tracking
4. Add detailed retry logging with reasons
5. Display retry summary in final statistics

## Completed Features
- Full --explain metadata processing with frontmatter support
- Code-aware chunking that respects programming structures
- Dry-run mode for testing without API calls
- Enhanced input validation with user-friendly error messages
- Remaining daily tokens/requests display after processing

## Project Overview
cereproc.py is a document processing tool that leverages Cerebras AI models to transform large documents through intelligent chunking and LLM processing. The tool supports:
- Multiple chunking strategies (text, semantic, markdown, code)
- Metadata extraction and validation
- Rate limiting and backoff strategies
- Atomic file writing with frontmatter preservation
- Comprehensive error handling and validation

## Architecture
- CLI framework: Fire
- Token counting: tiktoken with cl100k_base encoding
- Frontmatter handling: python-frontmatter
- Structured outputs: Cerebras JSON schema validation
- Logging: loguru for verbose mode debugging
</document_content>
</document>

<document index="10">
<source>README.md</source>
<document_content>
# cereproc.py

Process large documents by intelligently chunking them for Cerebras `qwen-3-coder-480b` processing with optimal performance and continuity.

## TLDR

**What it does**: Takes a large text file, splits it into model-sized chunks, sends each chunk to Cerebras for processing, and concatenates all responses into a single output file while maintaining logical continuity between chunks.

**Why it's useful**: Allows processing of documents larger than the 32K token input limit while preserving context and narrative flow through intelligent chunking and continuity examples.

## Quick Start

```bash
# Basic usage
python cereproc.py --input_data document.txt

# With custom prompt and chunking
python cereproc.py \
    --input_data large_doc.md \
    --output_data processed_doc.md \
    --file_prompt instructions.txt \
    --data_format markdown \
    --chunk_size 30000
```

## Code Structure

### Single-File Architecture
- **cereproc.py**: Complete implementation (~400-500 lines)
  - CLI interface via Fire
  - Chunking strategies (text/semantic/markdown/code)
  - Continuity system for cross-chunk context
  - Rate limit monitoring and adaptive backoff
  - Streaming API interaction with Cerebras

### Key Components

#### 1. Chunking Engine
```python
# Four chunking strategies
chunk_text_mode()      # Line-based greedy accumulation
chunk_semantic_mode()  # Semantic boundaries via semantic-text-splitter
chunk_markdown_mode()  # Markdown-aware splitting
chunk_code_mode()      # Code-aware with blank line bias
```

#### 2. Continuity System
```python
# Maintains context between chunks
extract_continuity_examples()  # Get last N tokens from prev input/output
build_continuity_block()       # Format context template
fit_continuity_to_budget()     # Truncate if needed for token limits
```

#### 3. Rate Limit Optimization
```python
parse_rate_limit_headers()    # Extract API quota info
calculate_backoff_delay()     # Adaptive delays for max throughput
process_chunk_with_streaming() # Streaming with rate monitoring
```

#### 4. Token Management
```python
# Precise token accounting throughout
qwen_tokenizer.encode()       # All token counting
calculate_completion_budget() # Per-chunk token allocation
validate_token_limits()       # Enforce 32K input / 40K output limits
```

## Functionality

### Core Features
- **Intelligent Chunking**: 4 modes (text/semantic/markdown/code) with token-accurate splitting
- **Continuity Preservation**: Last N tokens from previous chunks provide context
- **Rate Limit Optimization**: Parse API headers and adapt delays for maximum throughput
- **Streaming Processing**: Real-time output with progress tracking
- **Atomic Output**: Safe file writing with temporary files

### CLI Parameters
```bash
--input_data FILE         # Input document to process
--output_data FILE        # Output file (default: overwrite input)
--file_prompt FILE        # Instructions from file
--text_prompt TEXT        # Additional instruction text
--chunk_size INT          # Target tokens per chunk (default: 32000)
--max_tokens_ratio INT    # Completion budget % (default: 100)
--data_format MODE        # text|semantic|markdown|code (default: text)
--example_size INT        # Continuity example tokens (default: 200)
--temp FLOAT             # Model temperature (default: 0.7)
--top_p FLOAT            # Model top-p (default: 0.8)
```

### Dependencies
```python
# Required packages
cerebras-cloud-sdk-python  # API client
semantic-text-splitter     # Intelligent text chunking
qwen-tokenizer            # Token counting for Qwen models
fire                      # CLI framework
loguru                    # Simple logging
tenacity                  # Retry mechanisms
python-dotenv             # Environment management
```

## Performance Features

### Rate Limit Intelligence
- Monitors `x-ratelimit-remaining-requests` and `x-ratelimit-remaining-tokens` headers
- Calculates optimal delays to maximize throughput within limits
- Adaptive backoff when approaching quota limits

### Memory Efficiency
- Streams large files without loading entirely into memory
- Clears processed chunks promptly
- Caches tokenization results for repeated text

### Error Resilience
- Exponential backoff retry for transient failures
- Graceful handling of oversized chunks
- Continuity truncation when hitting token limits
- Atomic file operations prevent data loss

## Testing

```bash
# Test with provided data
cd testdata
./test.sh

# Manual testing
python cereproc.py --input_data testdata/test1.md --output_data out.txt --verbose
```

## Environment Setup

```bash
export CEREBRAS_API_KEY="csk-..."  # Required
export CEREPROC_LOG_LEVEL="INFO"  # Optional
```

## Use Cases

- **Document Processing**: Transform large reports, articles, or books
- **Code Analysis**: Process entire codebases with code-aware chunking
- **Content Generation**: Create long-form content with consistent style
- **Translation**: Translate large documents while preserving context
- **Summarization**: Generate summaries of lengthy documents

The tool prioritizes simplicity and performance while providing sophisticated chunking and continuity features for high-quality results on large documents.
</document_content>
</document>

<document index="11">
<source>TODO.md</source>
<document_content>
---
this_file: TODO.md
---

## Simple Quality Improvements - Version 1.3.0

### 1. Add Simple Progress Counter
- [ ] Show "Processing chunk X of Y..." for each chunk in non-verbose mode
- [ ] Keep existing progress bar, just add chunk counter above it
- [ ] Simple format: "📝 Processing chunk 3 of 7..."

### 2. Support Reading Prompt from stdin
- [ ] Allow file_prompt="-" to read prompt from stdin
- [ ] Useful for piping prompts: echo "Summarize this" | python cereproc.py input.txt output.txt -
- [ ] Simple implementation using sys.stdin.read()

### 3. Show File Size Summary
- [ ] After completion, show input vs output file sizes
- [ ] Format: "📊 Input: 245 KB → Output: 89 KB (36% of original)"
- [ ] Helps users understand compression/expansion from processing

## Rationale
These improvements are:
- **Simple**: Each can be implemented in <10 lines of code
- **Practical**: Add real value to user experience
- **Non-invasive**: Don't change core functionality
</document_content>
</document>

<document index="12">
<source>WORK.md</source>
<document_content>
---
this_file: WORK.md
---

# Current Work Progress

## Status
Version 1.2.2 - Core functionality refined and simplified

## Recently Completed
- Removed unnecessary complexity from codebase
- Clarified tool's core purpose and workflow
- Maintained focus on essential features only

## Core Workflow
1. Parse frontmatter and content
2. Chunk content using selected strategy
3. Explain metadata if --explain flag used
4. Process chunks through LLM
5. Save metadata + concatenated output

## Next Steps
Planning simple, practical improvements.
</document_content>
</document>

<document index="13">
<source>old/SPEC.md</source>
<document_content>
# cereproc.py Technical Specification

## Overview

`cereproc.py` is a Fire-based CLI tool that processes large input documents by intelligently chunking them into model-sized segments, sending each chunk to Cerebras `qwen-3-coder-480b` for processing, and concatenating the completions into a single output file. The tool emphasizes performance optimization within API rate limits while maintaining logical text continuity.

## Core Architecture

### Dependencies
- `python-dotenv`: Environment variable management
- `semantic-text-splitter`: Intelligent text segmentation
- `qwen-tokenizer`: Token counting and encoding/decoding
- `fire`: CLI interface framework
- `cerebras-cloud-sdk-python`: API client
- `loguru`: Simple logging
- `tenacity`: Retry mechanisms

### Design Principles
- Single-file implementation for simplicity
- Functional approach with minimal classes
- Aggressive performance optimization within rate limits
- Robust error handling without enterprise bloat
- Token-accurate accounting throughout

## API Constraints & Limits

### Model Specifications
- **Model**: `qwen-3-coder-480b` (overridable via `--model`)
- **Max completion tokens**: 40,000 (hard upper bound per request)
- **Max input tokens**: 32,000 (prompt + chunk + continuity additions)
- **Tokenization**: `qwen_tokenizer.encode(text)` for all counts

### Rate Limit Handling
- Parse response headers for rate limit information
- Implement adaptive backoff based on remaining quota
- Queue management for optimal throughput
- Graceful degradation when limits approached

## CLI Interface Specification

### Required Parameters
```bash
cereproc.py --input_data path/to/input.txt
```

### Optional Parameters
```bash
--output_data path/to/output.txt    # Default: overwrite input_data
--file_prompt path/to/prompt.txt    # Initial instruction file
--text_prompt "instruction text"    # Freeform instruction text
--chunk_size 32000                  # Target chunk size in tokens
--max_tokens_ratio 100              # Completion budget as % of chunk size
--data_format text|semantic|markdown|code  # Chunking strategy
--example_size 200                  # Continuity example token count
--temp 0.7                         # Model temperature
--top_p 0.8                        # Model top-p
--model qwen-3-coder-480b          # Model override
--verbose                          # Enable debug logging
```

## Prompt Assembly Strategy

### Base Prompt Construction
```python
def build_base_prompt(file_prompt_path: str | None, text_prompt: str | None) -> str:
    """Assemble base prompt from file and text components."""
    base_prompt = ""

    if file_prompt_path:
        base_prompt += read_file(file_prompt_path)

    base_prompt += "\n\n"

    if text_prompt:
        base_prompt += text_prompt

    return base_prompt
```

### Message Structure
```python
messages = [
    {"role": "system", "content": base_prompt},
    {"role": "user", "content": continuity_block + current_chunk}
]
```

## Chunking Strategies

### Text Mode (`data_format="text"`)
```python
def chunk_text_mode(content: str, chunk_size: int) -> list[Chunk]:
    """Line-based greedy accumulation respecting token limits."""
    chunks = []
    current_chunk = ""
    current_tokens = 0

    for line in content.splitlines(keepends=True):
        line_tokens = len(qwen_tokenizer.encode(line))

        if current_tokens + line_tokens > chunk_size and current_chunk:
            chunks.append(Chunk(current_chunk, current_tokens))
            current_chunk = line
            current_tokens = line_tokens
        else:
            current_chunk += line
            current_tokens += line_tokens

    if current_chunk:
        chunks.append(Chunk(current_chunk, current_tokens))

    return chunks
```

### Semantic Mode (`data_format="semantic"`)
```python
def chunk_semantic_mode(content: str, chunk_size: int) -> list[Chunk]:
    """Use semantic-text-splitter with token callback."""
    splitter = TextSplitter.from_callback(
        lambda s: len(qwen_tokenizer.encode(s)),
        chunk_size
    )

    chunk_texts = splitter.chunks(content)
    return [
        Chunk(text, len(qwen_tokenizer.encode(text)))
        for text in chunk_texts
    ]
```

### Markdown Mode (`data_format="markdown"`)
```python
def chunk_markdown_mode(content: str, chunk_size: int) -> list[Chunk]:
    """Use MarkdownSplitter with token callback."""
    splitter = MarkdownSplitter.from_callback(
        lambda s: len(qwen_tokenizer.encode(s)),
        chunk_size
    )

    chunk_texts = splitter.chunks(content)
    return [
        Chunk(text, len(qwen_tokenizer.encode(text)))
        for text in chunk_texts
    ]
```

### Code Mode (`data_format="code"`)
```python
def chunk_code_mode(content: str, chunk_size: int) -> list[Chunk]:
    """Code-aware chunking with fallback to text mode."""
    # Prefer semantic splitter if code-aware mode available
    # Fallback: bias splits at blank lines and ``` boundaries
    return chunk_text_mode_with_code_bias(content, chunk_size)
```

## Continuity System

### Continuity Block Template
```python
CONTINUITY_TEMPLATE = """Our current input text chunk is the immediate continuation of this input text chunk:

<previous_input>
(...){input_example}
</previous_input>

and the previous input chunk has been processed like so:

<previous_output>
(...){output_example}
</previous_output>

Please process our current input text analogically, and maintain logical and stylistic continuity of the text."""
```

### Example Extraction
```python
def extract_continuity_examples(
    prev_input: str,
    prev_output: str,
    example_size: int
) -> tuple[str, str]:
    """Extract last N tokens from previous input/output."""
    input_tokens = qwen_tokenizer.encode(prev_input)
    output_tokens = qwen_tokenizer.encode(prev_output)

    input_example_tokens = input_tokens[-example_size:]
    output_example_tokens = output_tokens[-example_size:]

    # Convert back to text (with fallback handling)
    input_example = decode_tokens_safely(input_example_tokens)
    output_example = decode_tokens_safely(output_example_tokens)

    return input_example, output_example
```

### Continuity Truncation
```python
def fit_continuity_to_budget(
    base_input_tokens: int,
    continuity_block: str,
    max_input_tokens: int = 32000
) -> str:
    """Truncate continuity examples to fit within token budget."""
    if base_input_tokens + len(qwen_tokenizer.encode(continuity_block)) <= max_input_tokens:
        return continuity_block

    available_tokens = max_input_tokens - base_input_tokens
    if available_tokens <= 0:
        return ""  # Drop continuity entirely

    # Proportionally reduce input/output examples
    return truncate_continuity_proportionally(continuity_block, available_tokens)
```

## Completion Token Budget

### Budget Calculation
```python
def calculate_completion_budget(
    chunk_tokens: int,
    continuity_tokens: int,
    max_tokens_ratio: int
) -> int:
    """Calculate max_completion_tokens for this chunk."""
    total_input_tokens = chunk_tokens + continuity_tokens

    # Calculate based on ratio
    requested_tokens = (chunk_tokens * max_tokens_ratio) // 100

    # Enforce hard limit
    max_completion_tokens = min(40000, requested_tokens)

    # Ensure total doesn't exceed theoretical limits
    return max_completion_tokens
```

## API Interaction & Rate Limiting

### Rate Limit Monitoring
```python
@dataclass
class RateLimitStatus:
    requests_remaining: int
    tokens_remaining: int
    reset_time: datetime

def parse_rate_limit_headers(response_headers: dict) -> RateLimitStatus:
    """Extract rate limit info from response headers."""
    return RateLimitStatus(
        requests_remaining=int(response_headers.get('x-ratelimit-remaining-requests', 0)),
        tokens_remaining=int(response_headers.get('x-ratelimit-remaining-tokens', 0)),
        reset_time=parse_reset_time(response_headers.get('x-ratelimit-reset'))
    )
```

### Adaptive Backoff
```python
def calculate_backoff_delay(rate_status: RateLimitStatus, chunk_tokens: int) -> float:
    """Calculate optimal delay based on rate limit status."""
    if rate_status.tokens_remaining < chunk_tokens:
        # Wait until reset
        return (rate_status.reset_time - datetime.now()).total_seconds()

    if rate_status.requests_remaining < 10:
        # Conservative delay when requests running low
        return 2.0

    return 0.1  # Minimal delay for optimal throughput
```

### Streaming Response Handler
```python
def process_chunk_with_streaming(
    messages: list[dict],
    max_completion_tokens: int,
    temp: float,
    top_p: float
) -> tuple[str, RateLimitStatus]:
    """Process single chunk with streaming response."""

    stream = client.chat.completions.create(
        messages=messages,
        model=model,
        stream=True,
        max_completion_tokens=max_completion_tokens,
        temperature=temp,
        top_p=top_p
    )

    response_text = ""
    rate_status = None

    for chunk in stream:
        if chunk.choices[0].delta.content:
            response_text += chunk.choices[0].delta.content

        # Extract rate limit info from headers if available
        if hasattr(chunk, 'headers'):
            rate_status = parse_rate_limit_headers(chunk.headers)

    return response_text, rate_status
```

## Error Handling & Resilience

### Retry Strategy
```python
@tenacity.retry(
    stop=tenacity.stop_after_attempt(3),
    wait=tenacity.wait_exponential(multiplier=1, min=1, max=60),
    retry=tenacity.retry_if_exception_type((ConnectionError, TimeoutError))
)
def make_api_request_with_retry(messages, **kwargs):
    """API request with exponential backoff retry."""
    return client.chat.completions.create(messages=messages, **kwargs)
```

### Validation & Edge Cases
```python
def validate_inputs(args) -> None:
    """Validate all input parameters and environment."""
    if not os.getenv('CEREBRAS_API_KEY'):
        raise ValueError("CEREBRAS_API_KEY environment variable required")

    if not Path(args.input_data).exists():
        raise FileNotFoundError(f"Input file not found: {args.input_data}")

    if args.chunk_size > 32000:
        logger.warning("chunk_size > 32000 may cause API errors")

    if args.example_size > args.chunk_size // 2:
        logger.warning("example_size very large relative to chunk_size")
```

## Output Assembly & File Operations

### Atomic File Writing
```python
def write_output_atomically(content: str, output_path: Path) -> None:
    """Write output using temporary file for atomicity."""
    temp_path = output_path.with_suffix(output_path.suffix + '.tmp')

    try:
        temp_path.write_text(content, encoding='utf-8')
        temp_path.replace(output_path)  # Atomic on most filesystems
    except Exception as e:
        if temp_path.exists():
            temp_path.unlink()
        raise e
```

### Progress Tracking
```python
def process_all_chunks(chunks: list[Chunk], **kwargs) -> str:
    """Process all chunks with progress tracking and rate limiting."""
    results = []

    for i, chunk in enumerate(chunks):
        logger.info(f"Processing chunk {i+1}/{len(chunks)} ({chunk.token_count} tokens)")

        # Build messages with continuity
        messages = build_messages_for_chunk(chunk, i, results)

        # Calculate budget
        max_tokens = calculate_completion_budget(chunk.token_count, ...)

        # Process with rate limiting
        result, rate_status = process_chunk_with_streaming(messages, max_tokens, ...)
        results.append(result)

        # Adaptive delay based on rate limits
        if i < len(chunks) - 1:  # Don't delay after last chunk
            delay = calculate_backoff_delay(rate_status, chunks[i+1].token_count)
            if delay > 0:
                logger.debug(f"Rate limit delay: {delay:.1f}s")
                time.sleep(delay)

    return "".join(results)
```

## Data Structures

### Core Data Classes
```python
@dataclass
class Chunk:
    text: str
    token_count: int

@dataclass
class ProcessingContext:
    base_prompt: str
    chunks: list[Chunk]
    example_size: int
    max_tokens_ratio: int
    model_params: dict
```

## Testing Strategy

### Test Data Structure
```
testdata/
├── test1.md          # Primary test document
├── test.sh           # Test execution script
├── expected_out.txt  # Expected output for validation
└── prompts/
    └── test_prompt.txt
```

### Test Script Template
```bash
#!/bin/bash
# testdata/test.sh

python cereproc.py \
    --input_data testdata/test1.md \
    --output_data testdata/actual_out.txt \
    --file_prompt testdata/prompts/test_prompt.txt \
    --data_format markdown \
    --verbose

# Compare output
diff testdata/expected_out.txt testdata/actual_out.txt
```

## Performance Optimizations

### Token Caching
- Cache tokenization results for repeated text
- Avoid re-tokenizing unchanged continuity examples

### Batch Optimization
- Process multiple chunks in parallel when rate limits allow
- Pipeline chunk preparation while previous chunk processes

### Memory Management
- Stream large file reading to avoid memory spikes
- Clear processed chunk data promptly

## Configuration & Environment

### Environment Variables
```bash
CEREBRAS_API_KEY=csk-...           # Required: API authentication
CEREPROC_LOG_LEVEL=INFO           # Optional: logging level
CEREPROC_MAX_RETRIES=3            # Optional: retry attempts
```

### Default Configuration
```python
DEFAULT_CONFIG = {
    'chunk_size': 32000,
    'max_tokens_ratio': 100,
    'data_format': 'text',
    'example_size': 200,
    'temp': 0.7,
    'top_p': 0.8,
    'model': 'qwen-3-coder-480b'
}
```

## Implementation Notes

### Token Handling Edge Cases
- Handle tokenizer decode failures gracefully
- Fall back to character-based approximation when decode unavailable
- Validate token counts against actual API consumption

### Continuity Logic
- First chunk: no continuity block
- Subsequent chunks: extract examples from previous input/output
- Truncation: reduce input/output examples proportionally

### Error Recovery
- API failures: retry with exponential backoff
- Rate limiting: adaptive delays based on headers
- Oversized chunks: process individually with warning
- Empty input: create empty output file

This specification provides the complete technical foundation for implementing `cereproc.py` as a robust, performant, and simple CLI tool focused on its core mission of intelligent document processing through the Cerebras API.
</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/cerebrate-file/old/cereproc.py
# Language: python

import os
import sys
import tempfile
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
import fire
import frontmatter
import json
from dotenv import load_dotenv
from loguru import logger
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from tqdm import tqdm
from qwen_tokenizer import get_tokenizer
from semantic_text_splitter import TextSplitter, MarkdownSplitter
from cerebras.cloud.sdk import Cerebras
import cerebras.cloud.sdk
import re

class Chunk:
    """Represents a text chunk with its token count."""

class RateLimitStatus:
    """Rate limit information from API response headers."""
    def __post_init__((self)):

class ProcessingState:
    """Tracks state across chunk processing."""

def __post_init__((self)):

def encode_text((text: str)) -> List[int]:
    """ Encode text to tokens with fallback handling...."""

def decode_tokens_safely((tokens: List[int])) -> str:
    """ Decode tokens back to text with fallback handling...."""

def parse_frontmatter_content((content: str)) -> Tuple[Dict[str, Any], str]:
    """ Parse frontmatter from content using python-frontmatter...."""

def check_metadata_completeness((metadata: Dict[str, Any])) -> Tuple[bool, List[str]]:
    """ Check if metadata contains all required fields...."""

def setup_logging((verbose: bool = False)) -> None:
    """Configure Loguru logging with appropriate verbosity."""

def validate_environment(()) -> None:
    """Validate required environment variables and dependencies."""

def validate_inputs((
    input_data: str,
    chunk_size: int,
    sample_size: int,
    max_tokens_ratio: int,
    data_format: str = "text",
)) -> None:
    """Validate CLI input parameters with user-friendly error messages."""

def read_file_safely((file_path: str)) -> str:
    """Read file content with error handling."""

def build_base_prompt((
    file_prompt: Optional[str], text_prompt: Optional[str]
)) -> Tuple[str, int]:
    """ Assemble base prompt from file and text components...."""

def chunk_text_mode((content: str, chunk_size: int)) -> List[Chunk]:
    """ Line-based greedy accumulation respecting token limits...."""

def chunk_semantic_mode((content: str, chunk_size: int)) -> List[Chunk]:
    """ Use semantic-text-splitter with token callback...."""

def chunk_markdown_mode((content: str, chunk_size: int)) -> List[Chunk]:
    """ Use MarkdownSplitter with token callback...."""

def chunk_code_mode((content: str, chunk_size: int)) -> List[Chunk]:
    """ Code-aware chunking that respects code structure boundaries...."""

def is_good_split_point((line_idx: int, line: str)) -> bool:
    """Determine if this is a good place to split chunks."""

def extract_continuity_examples((
    prev_text: str, prev_tokens: List[int], sample_size: int
)) -> str:
    """ Extract last N tokens from previous text as continuity example...."""

def build_continuity_block((input_example: str, output_example: str)) -> str:
    """ Build continuity block using exact template from spec...."""

def fit_continuity_to_budget((
    continuity_block: str,
    base_input_tokens: int,
    max_input_tokens: int = MAX_CONTEXT_TOKENS,
)) -> str:
    """ Truncate continuity examples to fit within token budget...."""

def create_chunks((content: str, data_format: str, chunk_size: int)) -> List[Chunk]:
    """ Create chunks using the specified strategy...."""

def calculate_completion_budget((chunk_tokens: int, max_tokens_ratio: int)) -> int:
    """ Calculate max_completion_tokens for this chunk...."""

def prepare_chunk_messages((
    base_prompt: str,
    chunk: Chunk,
    continuity_block: str,
    base_prompt_tokens: int,
    metadata: Optional[Dict[str, Any]] = None,
)) -> Tuple[List[Dict[str, str]], int]:
    """ Prepare chat messages for API call with token validation...."""

def parse_rate_limit_headers((headers: Dict[str, str])) -> RateLimitStatus:
    """ Extract rate limit info from response headers...."""

def calculate_backoff_delay((
    rate_status: RateLimitStatus,
    next_chunk_tokens: int,
    processing_state: Optional[object] = None,
)) -> float:
    """ Calculate optimal delay based on rate limit status with multi-instance safety...."""

def explain_metadata_with_llm((
    client: "Cerebras",
    existing_metadata: Dict[str, Any],
    first_chunk_text: str,
    model: str,
    temp: float,
    top_p: float,
)) -> Dict[str, Any]:
    """ Use structured outputs to generate missing metadata fields...."""

def make_cerebras_request((
    client: Cerebras,
    messages: List[Dict[str, str]],
    model: str,
    max_completion_tokens: int,
    temperature: float,
    top_p: float,
)) -> Tuple[str, RateLimitStatus]:
    """ Make streaming request to Cerebras API with retry logic...."""

def write_output_atomically((
    content: str, output_path: str, metadata: Optional[Dict[str, Any]] = None
)) -> None:
    """Write output using temporary file for atomicity, optionally preserving frontmatter."""

def run((
    input_data: str,
    output_data: Optional[str] = None,
    file_prompt: Optional[str] = None,
    prompt: Optional[str] = None,
    chunk_size: int = DEFAULT_CHUNK_SIZE,
    max_tokens_ratio: int = 100,
    data_format: str = "markdown",
    sample_size: int = 200,
    temp: float = 0.7,
    top_p: float = 0.8,
    model: str = "qwen-3-coder-480b",
    verbose: bool = False,
    explain: bool = False,
    dry_run: bool = False,
)) -> None:
    """ Process large documents by chunking for Cerebras qwen-3-coder-480b...."""


<document index="14">
<source>package.toml</source>
<document_content>
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows 
</document_content>
</document>

<document index="15">
<source>pyproject.toml</source>
<document_content>
# this_file: pyproject.toml
#==============================================================================
# CEREBRATE-FILE PACKAGE CONFIGURATION
# This pyproject.toml defines the package metadata, dependencies, build system,
# and development environment for the cerebrate-file package.
#==============================================================================

#------------------------------------------------------------------------------
# PROJECT METADATA
# Core package information used by PyPI and package managers.
#------------------------------------------------------------------------------
[project]
name = 'cerebrate-file' # Package name on PyPI
description = '' # Short description
readme = 'README.md' # Path to README file
requires-python = '>=3.10' # Minimum Python version
keywords = [
] # Keywords for PyPI search
dynamic = ["version"] # Fields set dynamically at build time

# PyPI classifiers for package categorization
classifiers = [
    'Development Status :: 4 - Beta', # Package maturity level
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
    'Operating System :: OS Independent',
    'License :: OSI Approved :: MIT License',
    'Intended Audience :: Developers',
]

dependencies = [
]

# Author information
[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

# License information
[project.license]
text = 'MIT'

# Project URLs
[project.urls]
Documentation = 'https://github.com/twardoch/cerebrate-file#readme'
Issues = 'https://github.com/twardoch/cerebrate-file/issues'
Source = 'https://github.com/twardoch/cerebrate-file'

#------------------------------------------------------------------------------
# OPTIONAL DEPENDENCIES
# Additional dependencies for optional features, development, and testing.
#------------------------------------------------------------------------------
[project.optional-dependencies]

# Development tools
dev = [
    'pre-commit>=4.1.0', # Pre-commit hook manager - Keep pre-commit as is, update if newer pre-commit version is required
    'ruff>=0.9.7', # Linting and formatting - Keep ruff as is, update if newer ruff version is required
    'mypy>=1.15.0', # Type checking - Keep mypy as is, update if newer mypy version is required
    'absolufy-imports>=0.3.1', # Convert relative imports to absolute - Keep absolufy-imports as is, update if newer absolufy-imports version is required
    'pyupgrade>=3.19.1', # Upgrade Python syntax - Keep pyupgrade as is, update if newer pyupgrade version is required
    'isort>=6.0.1', # Sort imports - Keep isort as is, update if newer isort version is required
]

# Testing tools and frameworks
test = [
    'pytest>=8.3.4', # Testing framework - Keep pytest as is, update if newer pytest version is required
    'pytest-cov>=6.0.0', # Coverage plugin for pytest - Keep pytest-cov as is, update if newer pytest-cov version is required
    'pytest-xdist>=3.6.1', # Parallel test execution - Keep pytest-xdist as is, update if newer pytest-xdist version is required
    'pytest-benchmark[histogram]>=5.1.0', # Benchmarking plugin - Keep pytest-benchmark as is, update if newer pytest-benchmark version is required
    'pytest-asyncio>=0.25.3', # Async test support - Keep pytest-asyncio as is, update if newer pytest-asyncio version is required
    'coverage[toml]>=7.6.12',
]

docs = [
    "sphinx>=7.2.6",
    "sphinx-rtd-theme>=2.0.0",
    "sphinx-autodoc-typehints>=2.0.0",
    "myst-parser>=3.0.0", # Markdown support in Sphinx
]

# All optional dependencies combined
all = [
]

#------------------------------------------------------------------------------
# COMMAND-LINE SCRIPTS
# Entry points for command-line executables installed with the package.
#------------------------------------------------------------------------------
[project.scripts]
# CLINAME = "cerebrate_file.__main__:main"

#------------------------------------------------------------------------------
# BUILD SYSTEM CONFIGURATION
# Defines the tools required to build the package and the build backend.
#------------------------------------------------------------------------------
[build-system]
# Hatchling is a modern build backend for Python packaging
# hatch-vcs integrates with version control systems for versioning
requires = [
    'hatchling>=1.27.0', # Keep hatchling as is, update if newer hatchling version is required
    'hatch-vcs>=0.4.0', # Keep hatch-vcs as is, update if newer hatch-vcs version is required
]
build-backend = 'hatchling.build' # Specifies Hatchling as the build backend


#------------------------------------------------------------------------------
# HATCH BUILD CONFIGURATION
# Configures the build process, specifying which packages to include and
# how to handle versioning.
#------------------------------------------------------------------------------
[tool.hatch.build]
# Include package data files
include = [
    "src/cerebrate_file/py.typed", # For better type checking support
    "src/cerebrate_file/data/**/*", # Include data files if any

]
exclude = ["**/__pycache__", "**/.pytest_cache", "**/.mypy_cache"]

[tool.hatch.build.targets.wheel]
packages = ["src/cerebrate_file"]
reproducible = true


# Version control system hook configuration
# Automatically updates the version file from git tags
[tool.hatch.build.hooks.vcs]
version-file = "src/cerebrate_file/__version__.py"

# Version source configuration for git-tag-based semver
[tool.hatch.version]
source = 'vcs' # Get version from git tags or other VCS info
raw-options = { local_scheme = "no-local-version" }

# Metadata handling configuration
[tool.hatch.metadata]
allow-direct-references = true # Allow direct references in metadata (useful for local dependencies)

#------------------------------------------------------------------------------
# UV PACKAGE MANAGER CONFIGURATION
# Configuration for uv package manager - provides faster installs and better resolution
#------------------------------------------------------------------------------
[tool.uv]
dev-dependencies = [
    "pre-commit>=4.1.0",
    "ruff>=0.9.7",
    "mypy>=1.15.0",
    "pytest>=8.3.4",
    "pytest-cov>=6.0.0",
]


#------------------------------------------------------------------------------
# DEVELOPMENT ENVIRONMENTS

[tool.hatch.envs.default]
features = ['dev', 'test', 'all']
dependencies = [
]

# Commands available in the default environment
[tool.hatch.envs.default.scripts]
# Run tests with optional arguments
test = 'pytest {args:tests}'
# Run tests with coverage reporting
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/cerebrate_file --cov=tests {args:tests}"
# Run type checking
type-check = "mypy src/cerebrate_file tests"
# Run linting and formatting
lint = ["ruff check src/cerebrate_file tests", "ruff format --respect-gitignore src/cerebrate_file tests"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore src/cerebrate_file tests", "ruff check --fix src/cerebrate_file tests"]
fix = ["ruff check --fix --unsafe-fixes src/cerebrate_file tests", "ruff format --respect-gitignore src/cerebrate_file tests"]

# Matrix configuration to test across multiple Python versions

[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

#------------------------------------------------------------------------------
# SPECIALIZED ENVIRONMENTS
# Additional environments for specific development tasks.
#------------------------------------------------------------------------------

# Dedicated environment for linting and code quality checks
[tool.hatch.envs.lint]
detached = true # Create a separate, isolated environment
features = ['dev'] # Use dev extras  dependencies 

# Linting environment commands
[tool.hatch.envs.lint.scripts]
# Type checking with automatic type installation
typing = "mypy --install-types --non-interactive {args:src/cerebrate_file tests}"
# Check style and format code
style = ["ruff check {args:.}", "ruff format --respect-gitignore {args:.}"]
# Format and fix style issues
fmt = ["ruff format --respect-gitignore {args:.}", "ruff check --fix {args:.}"]
fix = ["ruff check --fix --unsafe-fixes {args:.}", "ruff format --respect-gitignore {args:.}"]
# Run all ops
all = ["style", "typing", "fix"]

# Dedicated environment for testing
[tool.hatch.envs.test]
features = ['test'] # Use test extras as dependencies

# Testing environment commands
[tool.hatch.envs.test.scripts]
# Run tests in parallel
test = "python -m pytest -n auto {args:tests}"
# Run tests with coverage in parallel
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/cerebrate_file --cov=tests {args:tests}"
# Run benchmarks
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
# Run benchmarks and save results
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

# Documentation environment
[tool.hatch.envs.docs]
features = ['docs']

# Documentation environment commands
[tool.hatch.envs.docs.scripts]
build = "sphinx-build -b html docs/source docs/build"

# GitHub Actions workflow configuration
[tool.hatch.envs.ci]
features = ['test']


[tool.hatch.envs.ci.scripts]
test = "pytest --cov=src/cerebrate_file --cov-report=xml"


#------------------------------------------------------------------------------
# CODE QUALITY TOOLS
# Configuration for linting, formatting, and code quality enforcement.
#------------------------------------------------------------------------------

#------------------------------------------------------------------------------
# COVERAGE CONFIGURATION
# Settings for test coverage measurement and reporting.
#------------------------------------------------------------------------------

# Path mapping for coverage in different environments
[tool.coverage.paths]
cerebrate_file = ["src/cerebrate_file", "*/cerebrate-file/src/cerebrate_file"]
tests = ["tests", "*/cerebrate-file/tests"]

# Coverage report configuration
[tool.coverage.report]
# Lines to exclude from coverage reporting
exclude_lines = [
    'no cov', # Custom marker to skip coverage
    'if __name__ == .__main__.:', # Script execution guard
    'if TYPE_CHECKING:', # Type checking imports and code
    'pass', # Empty pass statements
    'raise NotImplementedError', # Unimplemented method placeholders
    'raise ImportError', # Import error handling
    'except ImportError', # Import error handling
    'except KeyError', # Common error handling
    'except AttributeError', # Common error handling
    'except NotImplementedError', # Common error handling
]

[tool.coverage.run]
source_pkgs = ["cerebrate_file", "tests"]
branch = true # Measure branch coverage (if/else statements)
parallel = true # Support parallel test execution
omit = [
    "src/cerebrate_file/__about__.py",
]

#------------------------------------------------------------------------------
# MYPY CONFIGURATION
# Configuration for type checking with mypy.
#------------------------------------------------------------------------------

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[[tool.mypy.overrides]]
module = ["tests.*"]
disallow_untyped_defs = false
disallow_incomplete_defs = false

#------------------------------------------------------------------------------
# PYTEST CONFIGURATION
# Configuration for pytest, including markers, options, and benchmark settings.
#------------------------------------------------------------------------------

[tool.pytest.ini_options]
addopts = "-v --durations=10 -p no:briefcase"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
console_output_style = "progress"
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
log_cli = true
log_cli_level = "INFO"
markers = [
    "benchmark: marks tests as benchmarks (select with '-m benchmark')",
    "unit: mark a test as a unit test",
    "integration: mark a test as an integration test",
    "permutation: tests for permutation functionality", 
    "parameter: tests for parameter parsing",
    "prompt: tests for prompt parsing",
]
norecursedirs = [
    ".*",
    "build",
    "dist", 
    "venv",
    "__pycache__",
    "*.egg-info",
    "_private",
]
python_classes = ["Test*"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
testpaths = ["tests"]

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min",    # Minimum time
    "max",    # Maximum time
    "mean",   # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr",    # Inter-quartile range
    "ops",    # Operations per second
    "rounds", # Number of rounds
]

#------------------------------------------------------------------------------
# RUFF CONFIGURATION
# Configuration for Ruff, including linter and formatter settings.
#------------------------------------------------------------------------------ 

# Ruff linter and formatter configuration
[tool.ruff]
target-version = "py310"
line-length = 100

# Linting rules configuration
[tool.ruff.lint]
# Rule sets to enable - sensible defaults for most projects
select = [
    'E', # pycodestyle errors
    'W', # pycodestyle warnings
    'F', # pyflakes
    'I', # isort
    'N', # pep8-naming
    'UP', # pyupgrade
    'B', # flake8-bugbear
    'C4', # flake8-comprehensions
    'SIM', # flake8-simplify
    'RUF', # Ruff-specific rules
    'PT', # flake8-pytest-style
    'PTH', # flake8-use-pathlib
    'ARG', # flake8-unused-arguments
    'PLE', # pylint errors
    'PLW', # pylint warnings
]
# Rules to ignore (with reasons)
ignore = [
    'E501', # Line too long - handled by formatter
    'W503', # Line break before binary operator - conflicts with formatter
    'PLW0603', # Using the global statement - sometimes necessary
    'SIM102', # Nested if statements - sometimes more readable
]
# Rules that should not be automatically fixed
unfixable = [
    'F401', # Don't automatically remove unused imports - may be needed later
]
# Configure extend-exclude to ignore specific directories
extend-exclude = [".git", ".venv", "venv", "dist", "build"]

# isort configuration within Ruff
[tool.ruff.lint.isort]
known-first-party = ['cerebrate_file'] # Treat as first-party imports for sorting

# flake8-tidy-imports configuration within Ruff
[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = 'all' # Ban all relative imports for consistency

# Per-file rule exceptions
[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
'tests/**/*' = [
    'PLR2004', # Allow magic values in tests for readability
    'S101', # Allow assertions in tests
    'TID252'
    # Allow relative imports in tests for convenience
]

</document_content>
</document>

# File: /Users/adam/Developer/vcs/github.twardoch/pub/cerebrate-file/src/cerebrate_file/cerebrate_file.py
# Language: python

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
import logging

class Config:
    """Configuration settings for cerebrate_file."""

def process_data((
    data: List[Any],
    config: Optional[Config] = None,
    *,
    debug: bool = False
)) -> Dict[str, Any]:
    """Process the input data according to configuration."""

def main(()) -> None:
    """Main entry point for cerebrate_file."""


# File: /Users/adam/Developer/vcs/github.twardoch/pub/cerebrate-file/tests/test_package.py
# Language: python

import cerebrate_file

def test_version(()):
    """Verify package exposes version."""


</documents>