The simple `./testapi.py` script works and returns a TLDR of CHANGELOG.md as requested

```
[14:04:48] $ ./testapi.py

# Changelog

## [Unreleased] - 2025-09-20

### Changed
- Build: Migrated `tool.uv.dev-dependencies` to `[dependency-groups].dev`
...
```

But using the full `cerebrate-file` tool (the main thing in this repo), we get: 

```
[14:05:01] $ cerebrate-file -i CHANGELOG.md -o CHANGELOG2.md -c 1024 -p "Slightly compress this CHANGELOG: only keep relevant facts, eliminate fluff"
Processing: CHANGELOG.md
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”   0%2025-11-14 14:05:11 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 1 returned zero tokens. Request diagnostics: {'chunk_index': 1, 'chunk_tokens': 9, 'total_input_tokens': 25, 'max_completion_tokens': 9, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172761, 'rate_tokens_remaining': 1500000, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  14%2025-11-14 14:05:12 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 2 returned zero tokens. Request diagnostics: {'chunk_index': 2, 'chunk_tokens': 18, 'total_input_tokens': 34, 'max_completion_tokens': 18, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172760, 'rate_tokens_remaining': 1499960, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  29%2025-11-14 14:05:14 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 3 returned zero tokens. Request diagnostics: {'chunk_index': 3, 'chunk_tokens': 735, 'total_input_tokens': 751, 'max_completion_tokens': 735, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172759, 'rate_tokens_remaining': 1499902, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  43%2025-11-14 14:05:16 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 4 returned zero tokens. Request diagnostics: {'chunk_index': 4, 'chunk_tokens': 795, 'total_input_tokens': 811, 'max_completion_tokens': 795, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172758, 'rate_tokens_remaining': 1498420, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  57%2025-11-14 14:05:18 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 5 returned zero tokens. Request diagnostics: {'chunk_index': 5, 'chunk_tokens': 769, 'total_input_tokens': 785, 'max_completion_tokens': 769, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172757, 'rate_tokens_remaining': 1496834, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•¸â”â”â”â”â”â”â”â”â”â”â”  71%2025-11-14 14:05:20 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 6 returned zero tokens. Request diagnostics: {'chunk_index': 6, 'chunk_tokens': 565, 'total_input_tokens': 581, 'max_completion_tokens': 565, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172756, 'rate_tokens_remaining': 1495317, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•ºâ”â”â”â”â”  86%2025-11-14 14:05:21 | WARNING  | cerebrate_file.cerebrate_file:process_document:292 - Chunk 7 returned zero tokens. Request diagnostics: {'chunk_index': 7, 'chunk_tokens': 454, 'total_input_tokens': 470, 'max_completion_tokens': 454, 'response_tokens': 0, 'response_chars': 0, 'model': 'zai-glm-4.6', 'temperature': 0.98, 'top_p': 0.8, 'rate_requests_remaining': 172755, 'rate_tokens_remaining': 1494184, 'rate_headers_parsed': True}
ðŸ“„ CHANGELOG.md â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100%
âœ… Saved: CHANGELOG2.md (172,755 calls remaining)

âŒ Cerebras returned zero tokens for the document â€“ no data was written to CHANGELOG2.md.
   Input file remains unchanged: CHANGELOG.md
   API diagnostics (first 3 zero-output chunks):
    - Chunk 1: input=25 tokens, budget=9, model=zai-glm-4.6, temp=0.98, top_p=0.8, requests_remaining=172761, tokens_remaining=1500000
    - Chunk 2: input=34 tokens, budget=18, model=zai-glm-4.6, temp=0.98, top_p=0.8, requests_remaining=172760, tokens_remaining=1499960
    - Chunk 3: input=751 tokens, budget=735, model=zai-glm-4.6, temp=0.98, top_p=0.8, requests_remaining=172759, tokens_remaining=1499902
    ... and 4 more zero-output chunks
2025-11-14 14:05:21 | ERROR    | cerebrate_file.cli:_report_zero_output_failure:734 - Aborting write because Cerebras returned zero tokens for %s -> %s | diagnostics=%s
~/Developer/vcs/github.twardoch/pub/cerebrate-file
```

The `cerebrate-file` tool used to work. However, Cerebras did some changes. 

TASK: Investigate carefully and systematically this failure and restore `cerebrate-file` to a fully working condition. Use a careful iterative approach. Do real-life tests on the Cerebras API, no need to mock anything. 

